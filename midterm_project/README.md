# Heart Disease Prediction Service


## 1. Project Overview

This repository contains a complete, production-ready machine learning project designed to predict the probability of heart disease. It demonstrates a full MLOps cycle, starting from raw data exploration and culminating in a containerized API, ready for cloud deployment.

The core goal is to build a reliable and reproducible service that takes patient health data as input and returns a prediction, making it a valuable tool for early-stage risk assessment.

### Key Features & Technologies
*   **Workflow:** Data Analysis â†’ Model Training â†’ API Service â†’ Containerization â†’ Cloud Deployment.
*   **Model:** The final model is selected after comparing Logistic Regression, Random Forest, and XGBoost.
*   **API Framework:** Flask is used to create a lightweight and robust prediction endpoint.
*   **Dependency Management:** `pyproject.toml` and `uv` for modern, fast, and reproducible package installation.
*   **Containerization:** `Docker` is used to package the application, ensuring it runs identically in any environment.

---

## 2. The Flow of the Project

This project is structured to follow a professional machine learning workflow, ensuring a clear separation between experimentation and production-ready code.

1.  **Phase 1: Experimentation (`notebook.ipynb`)**
    This is the "research lab." We use a Jupyter Notebook for its interactive nature to perform deep Exploratory Data Analysis (EDA), visualize data patterns, handle missing values, and experiment with different models to find the top performer and its best settings.

2.  **Phase 2: Production Training (`train.py`)**
    This is the "factory." We take the insights from the notebook and create an automated, repeatable script. This script loads the raw data, applies the exact same cleaning steps, and trains the winning model on the full dataset. Its only job is to produce the final, serialized model artifact (`heart_disease_model.pkl`).

3.  **Phase 3: Serving (`predict.py` & `Dockerfile`)**
    This is the "storefront." The `predict.py` script loads the trained model and exposes it via a Flask web server. The `Dockerfile` then packages this entire serviceâ€”including the Python runtime, all dependencies, and the application codeâ€”into a portable, self-contained unit called a Docker image.

---

## 3. The Function of Each Files

Understanding the role of each file is key to understanding the project.

| File | Purpose |
| :--- | :--- |
| **`notebook.ipynb`** | **The Research Notebook.** All initial data cleaning, visualization, feature engineering, and model comparison happens here. It's our interactive scratchpad for finding the best approach. |
| **`train.py`** | **The Model Trainer.** An automated script that takes the best model and parameters from the notebook, trains it on the full dataset, and saves the final `heart_disease_model.pkl` file. |
| **`predict.py`** | **The Prediction API.** A Flask web application that loads the trained model and exposes a `/predict` endpoint. It listens for `POST` requests containing patient data and returns a JSON prediction. |
| **`heart_disease.csv`** | **The Raw Data.** The original, untouched dataset used for training and analysis. |
| **`heart_disease_model.pkl`**| **The Trained Model Artifact.** A binary file containing the trained `DictVectorizer` and the final prediction model. This file is generated by `train.py`. |
| **`pyproject.toml`** | **The Dependency List.** The modern standard for defining the Python packages this project needs (e.g., `pandas`, `xgboost`, `flask`). |
| **`uv.lock`** | **The Environment Lock File.** A highly detailed file that lists the *exact* versions of all dependencies. Using this ensures that the project's environment is 100% reproducible every time. |
| **`Dockerfile`** | **The Container Blueprint.** A set of instructions for building a Docker image. It specifies the base OS, installs dependencies, copies the application files, and defines the command to run the service. |
| **`deploy.sh`** | **The Cloud Deployment Script.** An automation script to build the Docker image in the cloud and deploy it as a live web service on Google Cloud Run. |
| **`README.md`** | **The Project Manual.** This file, which provides a complete guide to understanding, running, and deploying the project. |

---

## 4. Reproducibility (in JupyterLab)

This guide will walk you through running the entire project from start to finish within a JupyterLab environment. All commands should be run in notebook cells.

### Step 1: Clone the Repository

First, get the project code onto your machine. Open a terminal and run:
```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

### Step 2: Set Up The Environment (First-Time Only)

This step creates a self-contained virtual environment and installs the exact dependencies needed for the project. **This only needs to be done once.**

In a new Jupyter Notebook cell, run the following:
```python
# Create a virtual environment folder named 'venv'
!python -m venv venv

# Install 'uv', our fast package installer
!venv/bin/python -m pip install uv

# Use 'uv' to install the exact package versions from the lock file.
# This guarantees that the environment is 100% reproducible.
!venv/bin/python -m uv pip sync uv.lock

print("âœ… Environment setup complete! You are ready to proceed.")
```

### Step 3: Data Analysis & Finding the Best Model

1.  From the JupyterLab file browser, open `notebook.ipynb`.
2.  Run all the cells from top to bottom. This will perform the EDA and model comparison.
3.  Scroll to the **"Overall Model Comparison"** section at the end. The output will tell you which model performed best. **Note down the winning model's name and its best parameters.** You will need these for the next step.

### Step 4: Training the Final Production Model

Now, we'll use our findings to train the final model.

1.  **âš ï¸ Critical Step: Edit `train.py`**. Go back to the JupyterLab file browser and open the `train.py` script.
2.  At the top of the script, update the `MODEL_NAME` and `BEST_PARAMS` variables with the winning model and parameters you noted down from the notebook.
3.  **Run the Training Script.** In a new notebook cell, run the following command. This executes the `train.py` script using the Python from your `venv`.

    ```python
    !venv/bin/python train.py
    ```
    After it finishes, you will see a new file, **`heart_disease_model.pkl`**, in your project directory. This is your trained model!

### Step 5: Serving the Model with the API

With the model trained, let's start the web service.

1.  **Run the Prediction Server.** In a new notebook cell, execute the command below. The `&` at the end runs the server as a **background process**, which is essential in a notebook so you can continue to run other cells.

    ```python
    # Start the server and save its logs to a file for debugging
    !venv/bin/python predict.py > server_logs.txt 2>&1 &

    print("ðŸš€ Prediction server started in the background on http://localhost:9696")
    ```

### Step 6: Testing the Live Prediction Service with `curl`

Your API is now running and waiting for requests. Let's send it some data to get a live prediction.

1.  **Send a `curl` Request.** In a new notebook cell, run the following command. This simulates an external application sending a patient's data to your API.

    ```python
    !curl -X POST "http://localhost:9696/predict" \
      -H "Content-Type: application/json" \
      -d '{
            "Age": 30, "Gender": "Male", "Blood Pressure": 160, "Cholesterol Level": 250,
            "Exercise Habits": "Low", "Smoking": "Yes", "Family Heart Disease": "Yes",
            "Diabetes": "No", "BMI": 32.0, "High Blood Pressure": "No",
            "Low HDL Cholesterol": "Yes", "High LDL Cholesterol": "Yes",
            "Stress Level": "Low", "Sleep Hours": 6, "Sugar Consumption": "Low",
            "Triglyceride Level": 200, "Fasting Blood Sugar": 130, "CRP Level": 4.0,
            "Homocysteine Level": 15.0
          }'
    ```
2.  **See the Result.** The cell will instantly output the prediction from your model in JSON format.

    *Example Output:*
    ```json
    {
      "has_heart_disease": true,
      "heart_disease_probability": 0.881234567
    }