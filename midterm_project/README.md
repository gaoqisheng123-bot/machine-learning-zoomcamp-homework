# Heart Disease Prediction Service

## Project Overview

This project builds a machine learning service to predict the probability of a patient having heart disease based on a set of health-related attributes. The goal is to provide a reliable, API-based tool that can be used for early risk assessment.

The project follows the following workflow:
1.  **Data Analysis:** In-depth exploratory data analysis (EDA) to understand the dataset.
2.  **Model Experimentation:** Training and comparing multiple models and parameter tuning (Logistic Regression, Random Forest, XGBoost) to find the best performer.
3.  **Production Training:** A standalone script to train the final, chosen model on the full dataset.
4.  **API Service:** A Flask-based web service that exposes the trained model via an API endpoint.
5.  **Containerization:** A `Dockerfile` to package the prediction service for reproducible and portable deployment.

---

## The Dataset

*   **Source:** [Kaggle Heart Disease Dataset](https://www.kaggle.com/datasets/oktayrdeki/heart-disease/data)
*   **Description:** The dataset contains 10,000 patient records with 21 columns. These columns include demographic information (`Age`, `Gender`), physiological measurements (`Blood Pressure`, `Cholesterol Level`), and lifestyle factors (`Smoking`, `Exercise Habits`).
*   **Objective:** The goal is to predict the `Heart Disease Status` column, which indicates whether a patient has heart disease ('Yes' or 'No').

---

## Project Structure

Here is a breakdown of the key files in this project:

```
â”œâ”€â”€ notebook.ipynb           # Jupyter Notebook for EDA and model experimentation.
â”œâ”€â”€ train.py                 # Python script to train the final model.
â”œâ”€â”€ predict.py               # Python script that serves the model via a Flask API.
â”œâ”€â”€ heart_disease.csv        # The raw dataset used for training.
â”œâ”€â”€ heart_disease_model.pkl  # The final trained model artifact (generated by train.py).
|
â”œâ”€â”€ pyproject.toml           # Defines the project's direct Python dependencies.
â”œâ”€â”€ uv.lock                  # A lock file for reproducible installation of dependencies.
|
â”œâ”€â”€ Dockerfile               # Blueprint for building a portable Docker container.
â””â”€â”€ README.md                # This instruction file.
```

---

## The End-to-End Workflow

This project is structured into distinct phases, moving from analysis to a live service.

1.  **Phase 1: Analysis in the Notebook (`notebook.ipynb`)**
    This is the "research" phase. We use the Jupyter notebook to freely explore the data, handle missing values, visualize distributions, and compare the performance of different machine learning models. The outcome of this phase is **knowledge**: we identify the best-performing model and its optimal hyperparameters.

2.  **Phase 2: Training the Production Model (`train.py`)**
    This is the "manufacturing" phase. We take the knowledge gained from the notebook and create a clean, automated script. This script loads the raw data, applies the exact same cleaning steps, and trains the winning model on the full training dataset. The outcome is a production-ready artifact: `heart_disease_model.pkl`.

3.  **Phase 3: Serving and Testing (`predict.py`)**
    This is the "deployment" phase. We create a script that loads the saved `heart_disease_model.pkl` file and wraps it in a Flask web server. This exposes our model to the outside world. We can then send it new patient data and get live predictions back.

---

## How to Run This Project (Using Jupyter Notebook / JupyterLab)

This guide will walk you through running the entire project from start to finish within a Jupyter environment. The commands are run inside notebook cells.

### Step 1: Environment Setup (First-Time Only)

Before you can run the code, you need to set up a dedicated environment with all the necessary libraries. This ensures that the project runs correctly without interfering with your other Python projects.

In a new notebook cell, run the following commands. This only needs to be done **once**.

```python
# Create a virtual environment folder named 'venv'
!python -m venv venv

# Install the 'uv' package manager, which is very fast
!venv/bin/python -m pip install uv

# Use 'uv' to install the exact package versions from the lock file
# This guarantees a reproducible environment.
!venv/bin/python -m uv pip sync uv.lock

print("âœ… Environment setup complete! You are ready to proceed.")
```

### Step 2: Analysis and Finding the Best Model

The first step in the workflow is to understand our data and find the best model.

1.  **Open `notebook.ipynb`**.
2.  **Run all the cells** from top to bottom.
3.  Scroll to the **"Overall Model Comparison"** section at the end. The output will tell you which model performed best on the validation data and what its best parameters are. **Write these down**, as you will need them in the next step.

    *Example Output:*
    ```
    The best model is: 'XGBoost' with a validation AUC of 0.8945
    ```

### Step 3: Training the Final Model

Now we will use our findings from the notebook to train the final, production-ready model.

1.  **Edit `train.py`:** Go back to the JupyterLab file browser and open the `train.py` script.
2.  **Update the Parameters:** At the top of the script, update the `MODEL_NAME` and `BEST_PARAMS` variables with the winning model and parameters you noted down from the notebook.

    *Example: If XGBoost won, you would change it to look like this:*
    ```python
    MODEL_NAME = 'XGBoost'
    BEST_PARAMS = {
        'learning_rate': 0.1,
        'max_depth': 5,
        'n_estimators': 150,
        # ... other XGBoost parameters
    }
    ```
3.  **Run the Training Script:** In a new notebook cell, run the following command. This executes the `train.py` script using the Python from the virtual environment you created in Step 1.

    ```python
    !venv/bin/python train.py
    ```
    After it finishes, you will see a new file, `heart_disease_model.pkl`, in your project directory. This is your trained model!

### Step 4: Serving the Model with the API

With the model trained, let's start the web service to make it available for predictions.

1.  **Run the Prediction Server:** In a new notebook cell, execute the command below. The `&` at the end runs the server as a **background process**, which is essential in a notebook so you can continue to run other cells.

    ```python
    # Start the server and save its logs to a file
    !venv/bin/python predict.py > server_logs.txt 2>&1 &

    print("ðŸš€ Prediction server started in the background.")
    print("You can now send requests to it.")
    ```

### Step 5: Testing the Live Prediction Service

Your API is now running and waiting for requests. Let's send it some data to get a prediction.

1.  **Send a `curl` Request:** In a new notebook cell, run the following command. This simulates an external application sending a patient's data to your API.

    ```python
    !curl -X POST "http://localhost:9696/predict" \
      -H "Content-Type: application/json" \
      -d '{
            "Age": 65, "Gender": "Male", "Blood Pressure": 160, "Cholesterol Level": 250,
            "Exercise Habits": "Low", "Smoking": "Yes", "Family Heart Disease": "Yes",
            "Diabetes": "Yes", "BMI": 32.0, "High Blood Pressure": "Yes",
            "Low HDL Cholesterol": "Yes", "High LDL Cholesterol": "Yes",
            "Stress Level": "High", "Sleep Hours": 5, "Sugar Consumption": "High",
            "Triglyceride Level": 200, "Fasting Blood Sugar": 130, "CRP Level": 4.0,
            "Homocysteine Level": 15.0
          }'
    ```
2.  **See the Result:** The cell will output the prediction from your model in JSON format.

    *Example Output:*
    ```json
    {
      "has_heart_disease": true,
      "heart_disease_probability": 0.881234567
    }
    ```

Congratulations! You have successfully run the entire machine learning workflow from data analysis to a live prediction.

---

## Containerization with Docker

For making this service truly portable and ready for production, a `Dockerfile` is included. This file contains all the instructions to package the prediction service into a lightweight, isolated container.

**Build the Docker Image:**
```bash
docker build -t heart-disease-service .
```

**Run the Docker Container:**
```bash
docker run -p 9696:9696 heart-disease-service
```
The service will now be running inside the container and can be tested using the same `curl` command as before.